{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Data Augmentation and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data augmentation\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(64),\n",
    "        transforms.CenterCrop(64),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)),  # Normalizing the images to [-1, 1]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Load your dataset\n",
    "dataset = datasets.ImageFolder(root=\"./training/images/\", transform=transform)\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(\n",
    "    dataset, batch_size=32, shuffle=True, num_workers=os.cpu_count()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Generator and Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(100, 512, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "\n",
    "# Discriminator model\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(512, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Models, Loss Function, and Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "device = torch.device(\"cpu\")\n",
    "G = Generator().to(device)\n",
    "D = Discriminator().to(device)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Optimizers\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/5][0/47] Loss_D: 1.5173 Loss_G: 3.3408 D(x): 0.5431 D(G(z)): 0.5782/0.0365\n",
      "[1/5][0/47] Loss_D: 0.0239 Loss_G: 16.2628 D(x): 0.9774 D(G(z)): 0.0000/0.0000\n",
      "[2/5][0/47] Loss_D: 0.1311 Loss_G: 10.4292 D(x): 0.8965 D(G(z)): 0.0016/0.0001\n",
      "[3/5][0/47] Loss_D: 1.0643 Loss_G: 5.3339 D(x): 0.8112 D(G(z)): 0.3830/0.0092\n",
      "[4/5][0/47] Loss_D: 0.5318 Loss_G: 2.4147 D(x): 0.6941 D(G(z)): 0.0711/0.1066\n"
     ]
    }
   ],
   "source": [
    "# Create instances of the models\n",
    "G = Generator().to(device)\n",
    "D = Discriminator().to(device)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Optimizers\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5  # Number of epochs. Adjust as needed\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, _) in enumerate(dataloader):\n",
    "        # Training the Discriminator\n",
    "        D.zero_grad()\n",
    "        real_images = images.to(device)\n",
    "        batch_size = real_images.size(0)\n",
    "        labels = torch.full((batch_size,), 1, dtype=torch.float, device=device)\n",
    "\n",
    "        output = D(real_images).view(-1)\n",
    "        loss_real = criterion(output, labels)\n",
    "        loss_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        # Generate fake images\n",
    "        noise = torch.randn(batch_size, 100, 1, 1, device=device)\n",
    "        fake_images = G(noise)\n",
    "        labels.fill_(0)\n",
    "\n",
    "        output = D(fake_images.detach()).view(-1)\n",
    "        loss_fake = criterion(output, labels)\n",
    "        loss_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "\n",
    "        loss_D = loss_real + loss_fake\n",
    "        D_optimizer.step()\n",
    "\n",
    "        # Training the Generator\n",
    "        G.zero_grad()\n",
    "        labels.fill_(1)  # The generator's goal is to fool the discriminator\n",
    "        output = D(fake_images).view(-1)\n",
    "        loss_G = criterion(output, labels)\n",
    "        loss_G.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        G_optimizer.step()\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(\n",
    "                f\"[{epoch}/{num_epochs}][{i}/{len(dataloader)}] \"\n",
    "                f\"Loss_D: {loss_D.item():.4f} Loss_G: {loss_G.item():.4f} \"\n",
    "                f\"D(x): {D_x:.4f} D(G(z)): {D_G_z1:.4f}/{D_G_z2:.4f}\"\n",
    "            )\n",
    "\n",
    "    # Optionally, save the models periodically or after training\n",
    "    # torch.save(G.state_dict(), f'generator_epoch_{epoch}.pth')\n",
    "    # torch.save(D.state_dict(), f'discriminator_epoch_{epoch}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the stats of our DCGAN model training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Loss_D (Discriminator Loss)**: This value indicates how well the discriminator is distinguishing between real and fake images. A lower Loss_D suggests better performance. However, if it's too low, it might mean the discriminator is overfitting or the generator is underperforming.\n",
    "\n",
    "2. **Loss_G (Generator Loss)**: This represents how well the generator is at creating images that the discriminator thinks are real. A lower Loss_G is generally better, indicating that the generator is improving.\n",
    "\n",
    "3. **D(x)**: This is the average output for the discriminator on real images. Closer to 1 is better, as it means the discriminator is correctly identifying real images as real.\n",
    "\n",
    "4. **D(G(z))**: This represents the discriminator's output on fake images. The first number (before the slash) is the value at the start of the iteration, and the second number (after the slash) is the value at the end. Ideally, you want the first number to be high (indicating the discriminator initially thinks the fake images are real) and the second number to be low (indicating that after the generator's update, its new images are more easily detected as fake by the discriminator)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(G.state_dict(), \"generator_final.pth\")\n",
    "torch.save(D.state_dict(), \"discriminator_final.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
